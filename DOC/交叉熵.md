# 交叉熵

一般用来求目标与预测值之间的差距。

## 1、信息量

假设$X$是一个离散型随机变量，其取值集合为$χ$，概率分布函数 $p(x)=Pr(X=x)，x∈χ$ ，则定义事件$X=x_0$的信息量为：
$$
I(x_0)=-log(p(x_0))
$$
$p(x_0)$的取值范围是[0,1]，图像如下：

![p(x0)的取值范围是 [0,1]](https://img-blog.csdn.net/20180125164333234?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHN5Y2NuaA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

## 2、熵

考虑另一个问题，对于某个事件，有$n$种可能性，每一种可能性都有一个概率$p(x_i)$

| 序号 |     事件     | 概率p |     信息量I     |
| :--: | :----------: | :---: | :-------------: |
|  A   | 电脑正常开机 |  0.7  | -log(p(A))=0.36 |
|  B   | 电脑无法开机 |  0.2  | -log(p(B))=1.61 |
|  C   |  电脑爆炸了  |  0.1  | -log(p(C))=2.30 |

熵用来表示所有信息量的期望:
$$
H(X)=-\sum_{i=1}^n p(x_i)log(p(x_i))=-\sum_{i=1}^n p(x_i)I(x_i)
$$
上面事件的熵为：
$$
\begin{eqnarray}
H(X)&=&-[p(A)log(p(A))+p(B)log(p(B))+p(C))log(p(C))]\\
&=&0.7\times 0.36+0.2\times 1.61+0.1\times 2.30\\
&=&0.804
\end{eqnarray}
$$

## 3、相对熵（KL散度）

在机器学习中，$P$往往用来表示样本的真实分布，比如$[1,0,0]$表示当前样本属于第一类。$Q$用来表示模型所预测的分布，比如$[0.7,0.2,0.1]$，直观的理解就是如果用P来描述样本，那么就非常完美。而用Q来描述样本，虽然可以大致描述，但是不是那么的完美。

KL散度的计算公式：
$$
D_{KL}(p||q)=\sum_{i=1}^np(x_i)log(\frac{p(x_i)}{q(x_i)}) \tag{3.1}
$$
$n$为事件的所有可能性。

$D_{KL}$的值越小，表示q分布和p分布越接近。

## 4、交叉熵

对式3.1变形可以得到：
$$
\begin{eqnarray}
D_{KL}(p||q) &=& \sum_{i=1}^np(x_i)log(p(x_i))-\sum_{i=1}^np(x_i)log(q(x_i))\\
&=& -H(p(x))+[-\sum_{i=1}^np(x_i)log(q(x_i))]
\end{eqnarray}
$$
在机器学习中，我们需要评估label和predicts之间的差距，使用KL散度刚刚好，即$D_{KL}(y||\hat{y})$，由于KL散度中的前一部分$-H(y)$不变，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用用交叉熵做loss，评估模型。

